----------------------------observation -----------------------------------

position=self.robot.GetBasePosition()
      self._observation = np.concatenate((self.robot.GetMotorAngles(), 
                                          self.robot.GetMotorVelocities(),
                                          self.robot.GetBaseLinearVelocity(), 
                                          self.robot.GetBaseAngularVelocity(), 
                                          [position[2]], 
                                          self.robot.GetBaseOrientation(), 
                                          self.robot.GetMotorTorques() 
                                          ))
                                          
                                          
--------------------------Reward-------------------------------------------

 def _reward_lr_course(self):
    """ Implement your reward function here. How will you improve upon the above? """
    """ Reward progress in the positive world x direction.  """
    current_base_position = self.robot.GetBasePosition()
    current_base_speed = self.robot.GetBaseLinearVelocity()
    roll_pitch_yaw=self.robot.GetBaseOrientationRollPitchYaw()
    current_torque=self.robot.GetMotorTorques()
    current_velocity=self.robot.GetMotorVelocities() 
    #print('torque',current_torque-self._last_motor_torque, np.sum(current_torque-self._last_motor_torque))
    #print('vel',current_velocity-self._last_motor_velocity,np.sum(current_velocity-self._last_motor_velocity))
    power_var=(current_torque)@(current_velocity)-self._last_motor_torque@self._last_motor_velocity
    #print('Power',power,np.sum((current_torque-self._last_motor_torque)@(current_velocity-self._last_motor_velocity)))
    #Weight
    w1=2
    w2=0.000001
    #print(np.shape(self._observation))
    forward_reward =w1*(current_base_position[0] - self._last_base_position[0])*np.cos(roll_pitch_yaw[2])+w2*power_var
    self._last_base_position = current_base_position
    self._last_motor_torque = current_torque
    self._last_motor_velocity = current_velocity
    # clip reward to MAX_FWD_VELOCITY (avoid exploiting simulator dynamics)
    if MAX_FWD_VELOCITY < np.inf:
      # calculate what max distance can be over last time interval based on max allowed fwd velocity
      max_dist = MAX_FWD_VELOCITY * (self._time_step * self._action_repeat)
      forward_reward = min( forward_reward, max_dist)

    return self._distance_weight * forward_reward 
